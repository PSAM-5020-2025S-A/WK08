{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 08\n",
        "\n",
        "Unsupervised Learning: Distances and Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
        "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/image_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import PIL.Image as PImage\n",
        "\n",
        "from data_utils import StandardScaler\n",
        "from data_utils import KMeansClustering, GaussianClustering\n",
        "from data_utils import object_from_json_url\n",
        "\n",
        "from image_utils import get_pixels, make_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost and Distance Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The concept of **_distance_** is something that we saw and used in previous exercises but didn't talk too much about.\n",
        "\n",
        "**_Distance_** is how we tell how close two data points are to each other, and is the basis for clustering, classification and regression algorithms.\n",
        "\n",
        "In classification, we learn how to label new data based on how \"close\" it is to our already-labeled training data. In regression, we find parameters to equations that make our line-of-best-fit \"close\" to all of the points in the dataset. In clustering, we group data points in a way that minimizes distances between points within a cluster, while maximizing the distances between clusters. Distance is also an important concept for recommendation systems where we want to calculate when someone's taste is close to someone else's.\n",
        "\n",
        "### 1D\n",
        "\n",
        "The concept of distance in one dimension is pretty easy to understand: it's how far two points on a line are to each other. Physically, we can think of $1D$ distance as the distance between runners in a race, or, we can even think of time as a one-dimensional space where we measure distance between events in seconds, or minutes, or days.\n",
        "\n",
        "<img src=\"./imgs/dist1d.jpg\" height=\"220px\" />\n",
        "\n",
        "Each point in 1-dimensional space is described with a single variable, and the distance between any two points is just the absolute value of their difference:\n",
        "\n",
        "$\\displaystyle D(x_0, x_1) = |x_0 - x_1|$<br>\n",
        "$\\displaystyle D(x_0, x_2) = |x_0 - x_2|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2D\n",
        "\n",
        "Two-dimensional distances, where we have $2$ variables to describe each of our points, is also pretty familiar to us. This can be the distance between two cities on a map, measured in angles of longitude and latitude, or distances between two points in Manhattan, measured in streets and avenues.\n",
        "\n",
        "<img src=\"./imgs/dist2d.jpg\" width=\"95%\" />\n",
        "\n",
        "We have $2$ variables for each of our points and we also have $2$ ways in which we can combine them to measure distances in $2D$. The first is called $L1$, or Manhattan, distance, and it's the sum of the distances in each of the separate dimensions.\n",
        "\n",
        "$\\displaystyle D_{L1}(x_0y_0, x_1y_1) = |x_0 - x_1| + |y_0 - y_1|$\n",
        "\n",
        "The other way of measuring distances in $2D$ is using the $L2$, or Euclidean, distance formula:\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0, x_2y_2) = \\sqrt{(x_0 - x_2)^2 + (y_0 - y_2)^2}$\n",
        "\n",
        "This is pretty easy to understand as distances on a map, but... what if $x$ is a variable for $height$ in our dataset, and $y$ is the variable for $ear\\ length$? The calculations are still valid. As long as we remember to normalize our data, we can use the $L1$ or $L2$ formulas to figure out how \"close\" our data points are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3D\n",
        "\n",
        "Three-dimensional points have $3$ variables that describe them, and, while less common, it's still easy to understand how to measure the distance between them. We could be talking about the distance between planets, or between atoms, or between a wifi router and a cellphone. As long as the points aren't on a plane, we need $3$ variables to describe them and measure the distance between them.\n",
        "\n",
        "<img src=\"./imgs/dist3d.jpg\" width=\"95%\" />\n",
        "\n",
        "We can extend the $L1$ and $L2$ distance formulas to work in $3D$:\n",
        "\n",
        "$\\displaystyle D_{L1}(x_0y_0z_0, x_1y_1z_1) = |x_0 - x_1| + |y_0 - y_1| + |z_0 - z_1|$\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0z_0, x_2y_2z_2) = \\sqrt{(x_0 - x_2)^2 + (y_0 - y_2)^2 + (z_0 - z_2)^2}$\n",
        "\n",
        "This works even when our $3$ variables aren't actually physical locations. If $x$ is a variable that keeps track of the number of rooms in a house, $y$ a variable for the age of the house, and $z$ the total area of the house, we can use the above formulas to measure how \"close\" two houses in our dataset are (after we normalize our data, of course).\n",
        "\n",
        "### N-Dimensions\n",
        "\n",
        "Most of the datasets we've seen so far already have more then $3$ features/dimensions... what then?\n",
        "\n",
        "Well... the $L1$ and $L2$ distance formulas can be used regardless of the number of features/dimensions in our dataset. We just keep adding parameters to our formula:\n",
        "\n",
        "$\\displaystyle D_{L1} = \\sum_{d}{|A_d - B_d|}$ (for all dimensions $d$)\n",
        "\n",
        "$\\displaystyle D_{L2} = \\sqrt{\\sum_{d}{(A_d - B_d)^2}}$ (for all dimensions $d$)\n",
        "\n",
        "So even when we have a dataset with $15$ or $20$ features/dimensions, we can still get some idea of how \"close\" two points in that dataset are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other Distance Formulas\n",
        "\n",
        "$L1$ and $L2$ are definitely the most widely used distance formulas in ML applications, but they aren't the only ones. Two other types of distances are:\n",
        "\n",
        "#### Cosine Similarity\n",
        "\n",
        "When we're dealing with datasets that are very sparse (there are more dimensions than points), and the $L1$ and $L2$ distances that separate the data points are really huge, we might want to measure the cosine similarity between two points instead.\n",
        "\n",
        "<img src=\"./imgs/distcos.jpg\" height=\"300px\" />\n",
        "\n",
        "In the drawing above, instead of measuring the direct distances between $x_0y_0$, $x_1y_1$ and $x_2y_2$, we can pick a separate reference point and measure the cosine of the angles formed by lines drawn from the reference point to each of the other points. We can see that angle $\\theta_{12}$ is smaller than $\\theta_{01}$ and $\\theta_{02}$, which means that $(x_1y_1, x_2y_2)$ is the pair of most similar points.\n",
        "\n",
        "Points with cosine values close to $1$ are in the same direction in space; points with cosine values close to $0$ are in perpendicular directions, and points with cosine values close to $-1$ are in opposite directions.\n",
        "\n",
        "$\\displaystyle cos(A, B) = \\frac{A \\cdot B}{ \\left|\\left|A\\right|\\right| \\left|\\left|B\\right|\\right|}$\n",
        "\n",
        "$\\displaystyle cos(x_0y_0, x_1y_1) = \\frac{x_0x_1 + y_0y_1}{\\sqrt{x_0^2+y_0^2} \\sqrt{x_1^2+y_1^2}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mahalanobis Distance\n",
        "\n",
        "This is useful for measuring distances between points that are part of a collection of points.\n",
        "\n",
        "<img src=\"./imgs/distmana-01.jpg\" height=\"250px\" />\n",
        "\n",
        "In the drawing above, if we only had the points $x_0y_0$, $x_1y_1$ and $x_2y_2$, we could use $L2$ distances and everything is fine:\n",
        "\n",
        "$\\displaystyle D_{L2}(x_0y_0, x_1y_1) > D_{L2}(x_0y_0, x_2y_2)$.\n",
        "\n",
        "But, if instead, $x_0y_0$, $x_1y_1$ and $x_2y_2$ are part of a collection of points with a well-defined average and standard deviation, like the image below, we should use something that makes more common distances shorter, and rarer distances larger.\n",
        "\n",
        "<img src=\"./imgs/distmana-02.jpg\" height=\"250px\" />\n",
        "\n",
        "In this case, where $x_0y_0$ and $x_2y_2$ are on the extremes of the distribution, we want the distance between them to be larger than the distance between $x_0y_0$ and $x_1y_1$, which happens along a more common direction of the data.\n",
        "\n",
        "In order to have $\\displaystyle D_{M}(x_0y_0, x_2y_2) > D_{M}(x_0y_0, x_1y_1)$, we have to take into account the distribution of our data: its mean, standard deviation and covariances. One way to do that is using the formula below:\n",
        "\n",
        "$\\displaystyle D_{M}(A, B) = \\sqrt{(A - B)^2 V_I}$.\n",
        "\n",
        "Where $V_I$ is the inverse of the covariance matrix of our points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "### Clustering\n",
        "\n",
        "#### More Wine ! üç∑üç∑üç∑\n",
        "\n",
        "Let's pretend we own an online wine store.\n",
        "\n",
        "Last week we created a model that predicts wine quality based on a bunch of its properties. We could use this model to figure out how much to pay suppliers for the wine, and how much to charge costumers.\n",
        "\n",
        "But, maybe this \"`quality`\" feature might not be something we want to share with our costumers. Even though it's based on data, it sounds abstract and subjective and would require explanations about our data and our process, which could create confusion.\n",
        "\n",
        "Using all features from the original dataset (`alcohol`, `acidity`, `density`, etc) might also not be very useful for costumers who want to buy new wines that are similar to ones that they have previously liked.\n",
        "\n",
        "What we can do instead is classify the wines into groups that take into account all of the features of the dataset, but present costumers with a more manageable amount of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recommendations\n",
        "\n",
        "What we're really hoping to have is a simple recommendation system for our costumers, where we can recommend wines based on previous wines they liked, without them having to know the $6$ features of the previous wines.\n",
        "\n",
        "There are a few ways of doing this, but the strategy we'll take is called clustering.\n",
        "\n",
        "### Clustering\n",
        "\n",
        "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis), or cluster analysis, is an example of an *unsupervised* learning method that groups items based on their many features and properties.\n",
        "\n",
        "We'll use it to divide our wines in such a way that wines in the same group, or *cluster*, are more similar to each other than to wines in other clusters.\n",
        "\n",
        "These clusters won't necessarily correlate directly to the features in our dataset, but will be computed using a combination of the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Learning\n",
        "\n",
        "The models that we've trained so far for doing regression and classification are considered *supervised* models. During training we give the model our input features, but also provide it with the *correct* values for the output signals. These output signals tend to be human-labeled values, and are sometimes called the *supervisory signals*.\n",
        "\n",
        "When fully-labeled training data is processed during training, we are hoping that the model learns to extrapolate what it *sees* in the labeled data to new, unseen, unlabeled instances of data with the same input features, but unknown output values.\n",
        "\n",
        "#### Supervised Classification:\n",
        "\n",
        "Given a set of initial data points with labels:<br>\n",
        "<img src=\"./imgs/classification-02.jpg\" width=\"620px\"/>\n",
        "\n",
        "We create a model that learns to assign labels to the original points:<br>\n",
        "<img src=\"./imgs/classification-03.jpg\" width=\"620px\"/>\n",
        "\n",
        "so that later we can assign correct labels to new data points:<br>\n",
        "<img src=\"./imgs/classification-04.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsupervised Learning\n",
        "\n",
        "Unlike supervised learning, unsupervised models learn patterns from unlabeled data. This means all of the features are considered input features, and there are no separate output features or signals. The idea is that by analyzing and processing data in specific ways, the model is able to build a concise representation of its features and create new ways of interpreting, visualizing or generating similar data.\n",
        "\n",
        "We can use unsupervised learning models to explore new datasets and try to simplify our data before we do any kind of supervised learning.\n",
        "\n",
        "We can also use supervised learning to build recommendation systems that learn how to group items by their many features or characteristics.\n",
        "\n",
        "The steps for training an unsupervised model should seem familiar:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Select variables and features to be considered\n",
        "5. Create a model\n",
        "6. Run model on input data and test data\n",
        "7. Measure error\n",
        "\n",
        "Even though it all looks familiar, that last step isn't very obvious.\n",
        "\n",
        "How do we measure error on a model that doesn't have a set of correct answers?\n",
        "\n",
        "Maybe *error* is not the right term, but we'll see how to define *metrics* to score and measure our unsupervised models.\n",
        "\n",
        "#### Unsupervised Clusterings:\n",
        "Since there are no correct labels, both of the following clusterings are valid!\n",
        "\n",
        "<img src=\"./imgs/clustering-00.jpg\" width=\"620px\"/>\n",
        "\n",
        "<img src=\"./imgs/clustering-01.jpg\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run it !\n",
        "\n",
        "### Preparing Data\n",
        "\n",
        "We'll load the same wine dataset as last week and normalize its features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wine_scaler = StandardScaler()\n",
        "wines_scaled = wine_scaler.fit_transform(wines_df)\n",
        "\n",
        "## 4. Select variables to be considered\n",
        "##    We're gonna drop the quality features to avoid re-clustering by quality\n",
        "features = wines_scaled.drop(columns=[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusterings\n",
        "\n",
        "Let's look at our first clustering algorithm:\n",
        "\n",
        "#### [K-means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means):\n",
        "Tries to separate the data into $k$ groups with similar properties. Requires the number of clusters to be determined beforehand, and the algorithm tries to minimize the difference between objects in a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters = 4\n",
        "\n",
        "## 5. Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## 6. Run the model on the data\n",
        "km_predicted = km_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plots\n",
        "\n",
        "Let's pick $2$ or $3$ variables to visualize our data and clusters.\n",
        "\n",
        "This could be any of our features, but let's look at the *covariances* table and pick features related to the highest covariance magnitudes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Look at covariances of features\n",
        "features.cov()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: look at max absolute value per column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The highest pairs are `density`/`sugar` and `alcohol`/`density`.\n",
        "\n",
        "We should visualize our clusters as a function of `density`, `sugar` and `alcohol`.\n",
        "\n",
        "Let's define a reusable function to help us with the plotting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_clusters(labels, clusters, title):\n",
        "  xl, yl, zl = labels[:3]\n",
        "  x = wines_scaled[xl]\n",
        "  y = wines_scaled[yl]\n",
        "  z = wines_scaled[zl]\n",
        "\n",
        "  # 2D\n",
        "  plt.scatter(x, y, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "  plt.title(f\"{title} clustering\")\n",
        "  plt.xlabel(xl)\n",
        "  plt.ylabel(yl)\n",
        "  plt.ylim([-2.2, 3])\n",
        "  plt.show()\n",
        "\n",
        "  plt.scatter(x, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "  plt.title(f\"{title} clustering\")\n",
        "  plt.xlabel(xl)\n",
        "  plt.ylabel(zl)\n",
        "  plt.ylim([-2.2, 3])\n",
        "  plt.show()\n",
        "\n",
        "  # 3D\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "  ax.scatter(x, y, z, c=clusters, marker='o', linestyle='', alpha=0.5)\n",
        "\n",
        "  ax.set_title(f\"{title} clustering\")\n",
        "  ax.set_xlabel(xl)\n",
        "  ax.set_ylabel(yl)\n",
        "  ax.set_zlabel(zl)\n",
        "  ax.set_ylim(-2.5, 8)\n",
        "  ax.set_zlim(-2.5, 2.5)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For plotting\n",
        "labels = [\"alcohol\", \"density\", \"sugar\"]\n",
        "clusters = km_predicted[\"clusters\"]\n",
        "\n",
        "plot_clusters(labels, clusters, \"K-Means\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Number of clusters\n",
        "\n",
        "Does the number of clusters affect clustering ?\n",
        "\n",
        "Change the variable and re-run clustering to see how the groupings change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: change n_clusters above and re-run clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "What changes ? Does one choice seem better than the others ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other Clustering Options\n",
        "\n",
        "There are many algorithms for clustering data, that differ in the assumptions they make about the data, and which parameters should be optimized.\n",
        "\n",
        "There's one method called [Gaussian Clustering](https://scikit-learn.org/stable/modules/mixture.html#mixture) that is similar to K-means, but it assumes that all features of our data can be modeled as [Gaussian distributions](https://en.wikipedia.org/wiki/Normal_distribution). During clustering, the algorithm incorporates information about the covariance structure of the data to try to determine mean and standard deviation values for each cluster.\n",
        "\n",
        "[Scikit-Learn](https://scikit-learn.org/) has implementation for this algorithm, and many more. It's called `GaussianClustering` and its constructor take the same parameters as the `KMeansClustering` constructor.\n",
        "\n",
        "Repeat steps $5$ and $6$ to create a model using `GaussianClustering` and run it on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters = 4\n",
        "\n",
        "## TODO: 5. Create Clustering object\n",
        "gc_model = ''\n",
        "\n",
        "## TODO: 6. Run the model on the data\n",
        "gc_predicted = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots for Gaussian Clustering results\n",
        "labels = [\"alcohol\", \"density\", \"sugar\"]\n",
        "clusters = gc_predicted[\"clusters\"]\n",
        "\n",
        "plot_clusters(labels, clusters, \"Gaussian\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Are the results any different than k-mean clustering ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoring\n",
        "\n",
        "Would be nice to have a way to measure how good these clusters actually are.\n",
        "\n",
        "It would help determine if we need more clusters, or if one method is actually better than the other.\n",
        "\n",
        "There are a couple of ways to do this. We'll look at three of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distance\n",
        "\n",
        "The first kind of scoring uses the sum of the distances between each point and its cluster's center as a metric.\n",
        "\n",
        "Each cluster's center is represented by the average values of all of the features of all of its members: $(\\overline{F_0}, \\overline{F_1}, \\overline{F_2}, ...)$. Once we know that we can use the L2-distance we saw above to calculate and accumulate the distances from each point to its cluster's center.\n",
        "\n",
        "A smaller cluster distance means that the cluster center is a good representation of its members.\n",
        "\n",
        "Luckily, our clustering models have a `distance_error()` function that can be used to report the distance error, after `fit()` has been called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KMeans distance error:\", km_model.distance_error())\n",
        "print(\"Gaussian distance error:\", gc_model.distance_error())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Likelihood\n",
        "\n",
        "The second way of scoring clusters treats each cluster as a potential normal distribution, and then calculates the likelihood that each point came from its cluster distribution.\n",
        "\n",
        "Values closer to zero mean that the clusters' statistical properties (mean, variation) are good estimators for the data.\n",
        "\n",
        "Our model objects also have a `likelihood_error()` function we can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KMeans likelihood error:\", km_model.likelihood_error())\n",
        "print(\"Gaussian likelihood error:\", gc_model.likelihood_error())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although somewhat related, the `distance` and `likelihood` calculations measure different things, and are in different units.\n",
        "\n",
        "We can't compare distances to likelihoods to draw any kind of conclusion.\n",
        "\n",
        "What we want to do is use either one of these metrics to select a clustering method and tune its parameters.\n",
        "\n",
        "### Balance\n",
        "\n",
        "A final metric we can consider when analyzing different clustering algorithms and strategies is to see how balanced the resulting clusters are. This isn't always important; we might have categories of items or events that are more common than others, and will produce unequal cluster groups.\n",
        "\n",
        "In other cases, where we know we want to have groups of similar sizes, this is a good metric to look at. For example, if we were to use the body measurement dataset for deciding how many sizes of bike helmets to produce, we should probably have sizes that cover similar portions of the population, and avoid very bespoke sizes that only fit few people.\n",
        "\n",
        "We compute `balance error` by summing the differences between our cluster sizes and the sizes of a perfectly balanced clustering. Once we have this sum, we scale it to get a number between $0$, for a perfectly balanced clustering, and $1$, for a most-unbalanced clustering.\n",
        "\n",
        "$\\displaystyle balance\\ error = \\frac{1}{2} \\left(\\frac{n}{n-1}\\right) \\sum_{i=1}^{n}{\\left|\\frac{C_i}{C_0 + C_1 + ... + C_n} - \\frac{1}{n}\\right|}$\n",
        "\n",
        "The $\\frac{C_i}{C_0 + C_1 + ... + C_n}$ terms are the sizes of our $n$ clusters expressed as the percentage of the total number of items in all clusters. The $\\frac{1}{n}$ term is the size of each cluster in a perfectly balanced clustering. We sum up these differences and scale it all by $\\frac{1}{2} \\left(\\frac{n}{n-1}\\right)$ to get a number between $0$ and $1$.\n",
        "\n",
        "We don't have to focus too much on this math right now. It's here for completeness and because it's good to practice reading an algorithm described as text, math equations and code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Balance Error\n",
        "\n",
        "Luckily this has also been implemented for us and we can get our model's `balance error` by calling the `balance_error()` function of our clustering object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"KMeans balance error:\", km_model.balance_error())\n",
        "print(\"Gaussian balance error:\", gc_model.balance_error())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of clusters\n",
        "\n",
        "If we consider the distance and balance metrics for the $2$ algorithms, it seems like `KMeansClustering` produces more dense and balanced clusters. This suggests that our features might not be normally distributed, or, our data is not a combination of gaussian features.\n",
        "\n",
        "Let's try different cluster numbers for the K-Means algorithm to see if there's a *better* way of clustering our wines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try 2 - 10 clusters for K-Means Clustering\n",
        "num_clusters = list(range(2,10))\n",
        "\n",
        "# collect distance, likelihood and balance errors\n",
        "dist_err = []\n",
        "like_err = []\n",
        "bala_err = []\n",
        "\n",
        "# get distance, likelihood and balance for different clustering sizes\n",
        "for n in num_clusters:\n",
        "  mm = KMeansClustering(n_clusters=n)\n",
        "  mm.fit_predict(features)\n",
        "  dist_err.append(mm.distance_error())\n",
        "  bala_err.append(mm.balance_error())\n",
        "\n",
        "# plot errors as function of number of clusters\n",
        "plt.plot(num_clusters, dist_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distance Error\")\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.ylim([0, 3])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(num_clusters, bala_err, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Balance Error\")\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.ylim([0, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Doesn't seem like the number of clusters affects the distance or balance errors for `KMeansClustering` too much.\n",
        "\n",
        "Let's re-cluster into $2$ clusters, just to see what that looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cluster\n",
        "n = 2\n",
        "m_model = KMeansClustering(n_clusters=n)\n",
        "km_predicted = m_model.fit_predict(features)\n",
        "\n",
        "# Plot\n",
        "labels = [\"alcohol\", \"density\", \"sugar\"]\n",
        "clusters = km_predicted[\"clusters\"]\n",
        "\n",
        "plot_clusters(labels, clusters, \"K-Mean\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis\n",
        "\n",
        "So, even though a higher number of clusters gives us slightly smaller distance error values, some of the clusters are really small and hard to find on the graphs.\n",
        "\n",
        "If this clustering is to be used for recommending wines to costumers, maybe using $2$ or $3$ clusters is a more sensible way of grouping our wines because the subtleties between $5$ categories of wine might be less easy to explain.\n",
        "\n",
        "Using $2$ or $3$ categories is probably more legible. The categories could be something like: `bold` and `sweet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Images\n",
        "\n",
        "Clustering can also be used to help analyze image and audio files.\n",
        "\n",
        "We can extract color information from an image by clustering its pixels by their RGB values. This can be used to get an estimate of the most \"important\" colors in an image. They're not the most common colors, necessarily, but the colors necessary to represent the image.\n",
        "\n",
        "This is called `color quantization` and is a kind of compression because we reduce the total number of colors in an image from a possible $16\\text{,}581\\text{,}375$ unique colors to $4$, $8$, $16$, etc... colors while preserving the overall appearance of the image. The calculated cluster centers become the color palette of the image, and we can re-color the image using only those colors.\n",
        "\n",
        "We start by loading an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mimg = PImage.open(\"./data/image/arara.jpg\")\n",
        "\n",
        "display(mimg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colors as features\n",
        "\n",
        "Since this process works on the pixels of a single image, we can think of each pixel as a measurement and its `R`, `G` and `B` values as its features.\n",
        "\n",
        "Before we put our image through clustering we should turn it into a `DataFrame` where the rows are the pixels and the columns are the `R`, `G`, `B` values.\n",
        "\n",
        "We can use the `pd.DataFrame.from_records()` function, but first we have to turn our pixel list into a list of objects:\n",
        "\n",
        "`[ [R,G,B], [R,G,B], [R,G,B], ... ]` -> `[{\"R\": R, \"G\": G, \"B\": B}, {\"R\": R, \"G\": G, \"B\": B}, ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create list of objects from list of lists\n",
        "pixelObj = [{\"R\":r, \"G\":g, \"B\":b} for r,g,b in get_pixels(mimg)]\n",
        "\n",
        "img_df = pd.DataFrame.from_records(pixelObj)\n",
        "\n",
        "img_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding/Scaling\n",
        "\n",
        "Not needed! All our features are already numbers, and they're all in the same units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot\n",
        "\n",
        "We just opened a new \"dataset\"... let's plot it.\n",
        "\n",
        "Since our features are the `R`, `G` and `B` channel values for each pixel, we can actually plot these in $3D$.\n",
        "\n",
        "Each of the features will place a point in `x`, `y`, `z` space, and the color of the point can be the color of the pixel.\n",
        "\n",
        "The only funny business here is that `pyplot` expects colors in the range $[0, 1]$ and not $[0, 255]$, so we have to convert those.\n",
        "\n",
        "We could use a `MinMaxScaler`, but since the inputs are already in a known range and the math is easy, we'll just use a comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert [0, 255] to [0, 1]\n",
        "c = [(r/255, g/255, b/255) for r,g,b in get_pixels(mimg)]\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(img_df[\"R\"], img_df[\"G\"], img_df[\"B\"], c=c, marker='o', linestyle='', alpha=0.5)\n",
        "ax.set_xlabel(\"R\")\n",
        "ax.set_ylabel(\"G\")\n",
        "ax.set_zlabel(\"B\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat\n",
        "\n",
        "Repeat the above process for a different image in the `./data/image` directory, or something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Visualize pixel color distributions for other images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster !\n",
        "\n",
        "Set up the `Clustering` object and run `fit_predtict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_clusters=8\n",
        "\n",
        "## Create Clustering object\n",
        "km_model = KMeansClustering(n_clusters=n_clusters)\n",
        "\n",
        "## Run the model on the pixel data\n",
        "km_predicted = km_model.fit_predict(img_df)\n",
        "\n",
        "km_predicted.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Un-Cluster / Group\n",
        "\n",
        "The `km_predicted` variable holds a `DataFrame` that is a mapping from pixel index to cluster index, but what we really want is to re-map our original pixels into the color palette made up of our cluster centers.\n",
        "\n",
        "The `KMeansClustering` object has a member variable called `cluster_centers_` that holds the centers of our clusters. We can use this to build a new pixel array for our image.\n",
        "\n",
        "Let's take a look at the cluster centers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "km_model.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have $8$ lists inside the `cluster_centers_` list. They should represent colors, but they're using floating point numbers right now, which will most likely give us troubles when we try to turn these into an image.\n",
        "\n",
        "Let's transform these into `int`s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using round() or int() to ensure our cluster centers are valid color values (ints)\n",
        "color_centers = [[round(r), round(g), round(b)] for r,g,b in km_model.cluster_centers_]\n",
        "\n",
        "print(color_centers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use these in a pixel array.\n",
        "\n",
        "Let's iterate through the `km_predicted[\"clusters\"]` and use those values to push their corresponding cluster center colors into a new pixel array.\n",
        "\n",
        "We go through `km_predicted[\"clusters\"]`, and if we see cluster $0$ we want to push `color_centers[0]` onto our pixel array, etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clustered_pxs = []\n",
        "\n",
        "# Iterate through the cluster list and append the right color for each pixel\n",
        "for gidx in km_predicted[\"clusters\"]:\n",
        "  clustered_pxs.append(color_centers[gidx])\n",
        "\n",
        "display(make_image(clustered_pxs, mimg.size[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
